{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb3bbd2",
   "metadata": {},
   "source": [
    "# Machine Translation Transformer Model\n",
    "\n",
    "\n",
    "## Introduction.\n",
    "In natural language processing (NLP), *transformer models* are a type of neural network (NN) architecture that have been used to achieve state-of-the-art performance on a variety of tasks, such as machine translation, text summarization, and question answering. Transformers are an attention-based encoder-decoder NN architecture. A transformer model takes as input a sequence of words (for example, a sentence or a document), and produces a new, transformed sequence of words as output (for example, a translated sentence or a summary of the input). The key innovation in transformer models is the attention mechanism, which allows the model to weigh the importance of different parts of the input when producing the output. The attention mechanism is a form of memory, which the model uses to \"remember\" what it has seen so far in the input, and use this information to inform its predictions for the remaining part of the input. Transformer models are also using a Multi-Headed Attention mechanism, where multiple attention mechanism heads are used in the same layer to improve the performance by allowing the model to attend to different part of the sequence simultaneously. In simple terms, Transformer models are neural networks that are good at understanding sequences of data (such as sentences or paragraphs) by paying attention to different parts of the input and \"remembering\" what it has seen so far, which helps it make better predictions about what comes next.\n",
    "\n",
    "Transformer models work by learning to map a sentence in one language (the *source language*) to a corresponding sentence in another language (the *target language*). The transformer model is trained on a large dataset of sentence pairs in the source and target languages. During training, the model is presented with a source sentence, and its task is to predict the corresponding target sentence. The transformer model works by *encoding* the source sentence into a fixed-length vector representation, called the *context vector*, which contains information about the meaning of the entire sentence. The model then *decodes* the context vector into a target sentence by generating words one at a time. The *attention mechanism* is the key component that allows the transformer model to achieve state-of-the-art performance on machine translation. The attention mechanism allows the model to \"pay attention\" to different parts of the source sentence when generating each word in the target sentence. This allows the model to \"remember\" what it has seen so far in the source sentence and use this information to inform its predictions for the remaining words in the target sentence. The Transformer's decoder uses multi-head attention, where the decoder attend to different parts of the source sentence at the same time, thus providing more information for a final output. In summary, transformer models for machine translation work by encoding a source sentence into a fixed-length context vector,\" and then use that context vector and the attention mechanism to decode it into the target sentence, word by word.\n",
    "\n",
    "\n",
    "### The Project at a Glance.\n",
    "In this project we will build from scratch and train an English-to-Italian attention-based transformer translator. Not only do transformer models represent the state-of-the art in machine translation and other NLP tasks, but this project also provides the opportunity of discussing a huge set of concepts relevant for NLP. We will dive deep in amny of these concepts and provide references for the curious reader to expand on related topics. To this end, this script can be considered a syllabus of recent NLP techniques.\n",
    "\n",
    "\n",
    "### Index.\n",
    "The notebook is divided into the following sections:\n",
    "1. Architecture of the Transformer\n",
    "2. Data Exploration & Text Normalization\n",
    "3. Vectorization\n",
    "4. Positional Encoding Matrix\n",
    "5. Positional Encoding Layer\n",
    "6. Transformer Building Blocks\n",
    "7. Transformer Encoder and Decoder\n",
    "8. Building a Transformer\n",
    "9. Preparing the Transformer Model for Training\n",
    "10. Training the Transformer\n",
    "11. Inference from the Transformer Model\n",
    "12. Improving the Model\n",
    "13. Comparing Model Performance\n",
    "\n",
    "\n",
    "### Working Environment.\n",
    "This tutorial was created using Python 3.9.15 and TensorFlow 2.10.\n",
    "\n",
    "\n",
    "### Acknowledgements.\n",
    "Some code snippets are taken by [this tutorial](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/) by Fran√ßois Chollet, and by [this tutorial](https://machinelearningmastery.com/building-transformer-models-with-attention-crash-course-build-a-neural-machine-translator-in-12-days/?utm_source=drip&utm_medium=email&utm_campaign=Build+a+neural+machine+translator+in+12+Days&utm_content=Build+a+neural+machine+translator+in+12+Days) by Adrian Tam. The transformer model's architecture is identical to the one described in the seminal article [\"Attention Is All You Need\" (Google; 2017)](https://arxiv.org/pdf/1706.03762.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
