{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f82561c6",
   "metadata": {},
   "source": [
    "## Section 05: Positional Encoding Layer\n",
    "\n",
    "In this section, we will make use of the positional encoding matrix to build a positional encoding layer in TensorFlow. The positional encoding layer is at the entry point of a transformer model. However, the Keras library does not provide us one. We thus create a custom layer to implement the positional encoding.\n",
    "\n",
    "This custom Keras layer takes three parameters:\n",
    "- *sequence_length*: the maximum length of the input sequence.\n",
    "- *vocab_size*: the size of the vocabulary used to generate the token embeddings.\n",
    "- *embed_dim*: the dimension of the embedding vector.\n",
    "\n",
    "The layer has two sub-layers:\n",
    "- *token_embeddings*: this is an Embedding layer from Keras, it converts the input integer tokens to D-dimensional float vectors (where D is equal to embed_dim).\n",
    "- *position_embeddings*: this is a matrix of hard-coded sine values that is used to add positional information to the token embeddings.\n",
    "\n",
    "When the layer is called on an input, it first generates the token embeddings using the token_embeddings layer, then it adds the position embeddings to the token embeddings and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46f700df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"Positional embedding layer. Assume tokenized input, transform into\n",
    "    embedding and returns positional-encoded output.\"\"\"\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequence_length: Input sequence length\n",
    "            vocab_size: Input vocab size, for setting up embedding matrix\n",
    "            embed_dim: Embedding vector size, for setting up embedding matrix\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim     # d_model in paper\n",
    "        # token embedding layer: Convert integer token to D-dim float vector\n",
    "        self.token_embeddings = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim, mask_zero=True\n",
    "        )\n",
    "        # positional embedding layer: a matrix of hard-coded sine values\n",
    "        matrix = pos_enc_matrix(sequence_length, embed_dim, n=10000)\n",
    "        self.position_embeddings = tf.constant(matrix, dtype=\"float32\")\n",
    " \n",
    "    def call(self, inputs):\n",
    "        \"\"\"Input tokens convert into embedding vectors then superimposed\n",
    "        with position vectors\"\"\"\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        return embedded_tokens + self.position_embeddings\n",
    " \n",
    "    # this layer is using an Embedding layer, which can take a mask\n",
    "    # see https://www.tensorflow.org/guide/keras/masking_and_padding#passing_mask_tensors_directly_to_layers\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.token_embeddings.compute_mask(*args, **kwargs)\n",
    " \n",
    "    def get_config(self):\n",
    "        # to make save and load a model using custom layer possible\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab4f410",
   "metadata": {},
   "source": [
    "This layer combines an embedding layer with position encoding. The embedding layer creates word embeddings, namely, converting an integer token label from the vectorized sentence into a vector that can carry the meaning of the word. With the embedding, you can tell how close in meaning the two different words are (see Section 04).\n",
    "\n",
    "The embedding output depends on the tokenized input sentence. But the positional encoding is a constant matrix as it depends only on the position. Hence you create a constant tensor for that at the time you created this layer. TensorFlow is smart enough to match the dimensions when you add the embedding output to the positional encoding matrix, in the call() function.\n",
    "\n",
    "Two additional functions are defined in the layer above. The compute_mask() function is passed on to the embedding layer. This is needed to tell which positions of the output are padded. This will be used internally by Keras. The get_config() function is defined to remember all the config parameters of this layer. This is a standard practice in Keras so that you remember all the parameters you passed on to the constructor and return them in get_config(), so the model can be saved and loaded.\n",
    "\n",
    "We will now pass the training dataset to the PositionalEmbedding layer and print one output.\n",
    "First, however, we need to re-load functions from Sections 3 and 4 and to re-create the training dataset (see Section 3 for details). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27c7f5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 36  13   4 ...   0   0   0]\n",
      " [ 42 518  34 ...   0   0   0]\n",
      " [  3 158  12 ...   0   0   0]\n",
      " ...\n",
      " [  3  24   8 ...   0   0   0]\n",
      " [298  32   4 ...   0   0   0]\n",
      " [  5 458 279 ...   0   0   0]], shape=(64, 20), dtype=int64)\n",
      "(64, 20, 512)\n",
      "tf.Tensor(\n",
      "[[ True  True  True ... False False False]\n",
      " [ True  True  True ... False False False]\n",
      " [ True  True  True ... False False False]\n",
      " ...\n",
      " [ True  True  True ... False False False]\n",
      " [ True  True  True ... False False False]\n",
      " [ True  True  True ... False False False]], shape=(64, 20), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "def format_dataset(eng, ita):\n",
    "    \"\"\"Take an English and a Italian sentence pair, convert into input and target.\n",
    "    The input is a dict with keys `encoder_inputs` and `decoder_inputs`, each\n",
    "    is a vector, corresponding to English and Italian sentences respectively.\n",
    "    The target is also vector of the Italian sentence, advanced by 1 token. All\n",
    "    vector are in the same length.\n",
    " \n",
    "    The output will be used for training the transformer model. In the model we\n",
    "    will create, the input tensors are named `encoder_inputs` and `decoder_inputs`\n",
    "    which should be matched to the keys in the dictionary for the source part\n",
    "    \"\"\"\n",
    "    eng = eng_vectorizer(eng)\n",
    "    ita = ita_vectorizer(ita)\n",
    "    source = {\"encoder_inputs\": eng,\n",
    "              \"decoder_inputs\": ita[:, :-1]} # between the [start] and [end] signals\n",
    "    target = ita[:, 1:] # between the [start] and [end] signals\n",
    "    return (source, target)\n",
    "  \n",
    "\n",
    "def make_dataset(pairs, batch_size=64):\n",
    "    \"\"\"Create TensorFlow Dataset for the sentence pairs\"\"\"\n",
    "    # aggregate sentences using zip(*pairs)\n",
    "    eng_texts, ita_texts = zip(*pairs)\n",
    "    # convert them into list, and then create tensors\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(ita_texts)))\n",
    "    return dataset.shuffle(2048) \\\n",
    "                  .batch(batch_size).map(format_dataset) \\\n",
    "                  .prefetch(16).cache()\n",
    "\n",
    "\n",
    "def pos_enc_matrix(L, d, n=10000):\n",
    "    assert d % 2 == 0, \"Output dimension needs to be an even integer\"\n",
    "    d2 = d//2\n",
    "    P = np.zeros((L, d))\n",
    "    k = np.arange(L).reshape(-1, 1)     # L-column vector\n",
    "    i = np.arange(d2).reshape(1, -1)    # d-row vector\n",
    "    denom = np.power(n, -i/d2)          # n**(-2*i/d)\n",
    "    args = k * denom                    # (L,d) matrix\n",
    "    P[:, ::2] = np.sin(args)\n",
    "    P[:, 1::2] = np.cos(args)\n",
    "    return P\n",
    "  \n",
    "  \n",
    "  \n",
    "import pickle \n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "with open(\"key_vals.pickle\", \"rb\") as fp:\n",
    "    key_vals = pickle.load(fp)\n",
    "\n",
    "with open(f\"vectorized_ENGvoc_{key_vals['vocab_size_eng']}_ITAvoc_{key_vals['vocab_size_ita']}_seqLen_{key_vals['seq_length']}.pickle\", \"rb\") as fp:\n",
    "    data = pickle.load(fp)\n",
    "\n",
    "# create new instances of the English and Italian vectorizers using the configurations that were saved previously.\n",
    "# The from_config() method allows for recreating the same TextVectorization layer from a previously saved configuration.\n",
    "eng_vectorizer = TextVectorization.from_config(data[\"engvec_config\"])\n",
    "eng_vectorizer.set_weights(data[\"engvec_weights\"])\n",
    "eng_vectorizer.set_vocabulary(data[\"engvec_vocabulary\"])\n",
    "ita_vectorizer = TextVectorization.from_config(data[\"itavec_config\"])\n",
    "ita_vectorizer.set_weights(data[\"itavec_weights\"])\n",
    "ita_vectorizer.set_vocabulary(data[\"itavec_vocabulary\"])\n",
    "  \n",
    "train_pairs = data[\"train\"]\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "\n",
    "# test the dataset\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    print(inputs[\"encoder_inputs\"])\n",
    "    embed_en = PositionalEmbedding(seq_length, key_vals[\"vocab_size_eng\"], embed_dim=512)\n",
    "    en_emb = embed_en(inputs[\"encoder_inputs\"])\n",
    "    print(en_emb.shape)\n",
    "    print(en_emb._keras_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45772839",
   "metadata": {},
   "source": [
    "The first tensor printed above is one batch (64 samples) of the vectorized input sentences, padded with zero to length seq_len (=20). Each token is an integer but will be converted into an embedding of dimension d (=512). Hence the shape of en_emb above is (batch size * seq_len * d) = (64, 20, 512).\n",
    "\n",
    "The last tensor printed above is the mask used (i.e., matches the input where the position is not zero). When we compute the accuracy, we have to remember the padded locations should not be counted.\n",
    "\n",
    "Finally, *pos_enc_matrix* and *PositionalEmbedding* were saved in the *positional_encoding* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f692f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
