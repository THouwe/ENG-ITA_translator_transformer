{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7528ebd",
   "metadata": {},
   "source": [
    "## Section 11: Inference from the Transformer Model\n",
    "\n",
    "Now, we are finally going to use the test set to see how good our trained model is.\n",
    "\n",
    "Because our model involves some custom made layers and functions in the model, we need to create a custom object scope to load the saved model.\n",
    "\n",
    "The transformer model can give you a token index. We need the vectorizer to look up the word that this index represents. We have to reuse the same vectorizer that you used in creating the dataset to maintain consistency.\n",
    "\n",
    "Create a loop to scan the generated tokens. In other words, do not use the model to generate the entire translated sentence but consider only the next generated word in the sentence until you see the end sentinel. The first generated word would be the one generated by the start sentinel. It is the reason you processed the target sentences this way in Section 2.\n",
    "\n",
    "The code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19c14cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "from positional_encoding import pos_enc_matrix, PositionalEmbedding\n",
    "# from learning_schedule import CustomSchedule\n",
    "from learning_params import CustomSchedule, masked_loss, masked_accuracy\n",
    "\n",
    "with open(\"key_values.pickle\", \"rb\") as fp:\n",
    "    key_vals = pickle.load(fp)\n",
    "\n",
    "with open(f\"vectorized_ENGvoc_{key_vals['vocab_size_eng']}_ITAvoc_{key_vals['vocab_size_ita']}_seqLen_{key_vals['seq_len']}.pickle\", \"rb\") as fp:\n",
    "    data = pickle.load(fp)    \n",
    "    \n",
    "with open(\"optimizer_values.pickle\", \"rb\") as fp:\n",
    "    optim_vals = pickle.load(fp)\n",
    "\n",
    "with open(\"customSchedule.pickle\", \"rb\") as fp:\n",
    "    schedule = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea54bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "eng_vectorizer = TextVectorization.from_config(data[\"engvec_config\"])\n",
    "eng_vectorizer.set_weights(data[\"engvec_weights\"])\n",
    "eng_vectorizer.set_vocabulary(data[\"engvec_vocabulary\"])\n",
    "ita_vectorizer = TextVectorization.from_config(data[\"itavec_config\"])\n",
    "ita_vectorizer.set_weights(data[\"itavec_weights\"])\n",
    "ita_vectorizer.set_vocabulary(data[\"itavec_vocabulary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18032f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {\"PositionalEmbedding\": PositionalEmbedding,\n",
    "                  \"CustomSchedule\": CustomSchedule,\n",
    "                  \"masked_loss\": masked_loss,\n",
    "                  \"masked_accuracy\": masked_accuracy}\n",
    "with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "    model = tf.keras.models.load_model(\"ENG-ITA-transformer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6dff0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_sub_accented(string):\n",
    "    \"\"\"Revert Italian accents\"\"\"\n",
    "    string = re.sub(r'aa', 'à', string)\n",
    "    string = re.sub(r'ee', 'è', string)\n",
    "    string = re.sub(r'ii', 'ì', string)\n",
    "    string = re.sub(r'oo', 'ò', string)\n",
    "    string = re.sub(r'uu', 'ù', string)\n",
    "    string = re.sub(r'aaa', 'á', string)\n",
    "    string = re.sub(r'eee', 'é', string)\n",
    "    string = re.sub(r'iii', 'í', string)\n",
    "    string = re.sub(r'ooo', 'ó', string)\n",
    "    string = re.sub(r'uuu', 'ú', string)  \n",
    "    return string\n",
    "\n",
    "def translate(sentence):\n",
    "    \"\"\"Create the translated sentence\"\"\"\n",
    "    enc_tokens = eng_vectorizer([sentence])\n",
    "    #lookup = list(ita_vectorizer.get_vocabulary())\n",
    "    lookup = list(data[\"itavec_vocabulary\"])\n",
    "    start_sentinel, end_sentinel = \"[start]\", \"[end]\"\n",
    "    output_sentence = [start_sentinel]\n",
    "    # generate the translated sentence word by word\n",
    "    for i in range(key_vals[\"seq_len\"]):\n",
    "        vector = ita_vectorizer([\" \".join(output_sentence)])\n",
    "        assert vector.shape == (1, key_vals[\"seq_len\"]+1)\n",
    "        dec_tokens = vector[:, :-1]\n",
    "        assert dec_tokens.shape == (1, key_vals[\"seq_len\"])\n",
    "        pred = model([enc_tokens, dec_tokens])\n",
    "        assert pred.shape == (1, key_vals[\"seq_len\"], key_vals[\"vocab_size_ita\"])\n",
    "        word = lookup[np.argmax(pred[0, i, :])]\n",
    "        word = re_sub_accented(word)\n",
    "        output_sentence.append(word)\n",
    "        if word == end_sentinel:\n",
    "            break\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b35ef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0:\n",
      "it is time to leave .\n",
      "== [start] è ora di partire . [end]\n",
      "-> [start] è andarsene . [end]\n",
      "\n",
      "Test 1:\n",
      "i like studying english .\n",
      "== [start] mi piace studiare inglese . [end]\n",
      "-> [start] a me piace studiare l inglese . [end]\n",
      "\n",
      "Test 2:\n",
      "do you want anything ?\n",
      "== [start] vuole qualcosa ? [end]\n",
      "-> [start] vuoi qualcosa ? [end]\n",
      "\n",
      "Test 3:\n",
      "do not tell my mom .\n",
      "== [start] non lo dire a mia mamma . [end]\n",
      "-> [start] non lo dire a mia mamma . [end]\n",
      "\n",
      "Test 4:\n",
      "what country were you born in ?\n",
      "== [start] in quale paese è nata ? [end]\n",
      "-> [start] in quale paese sei nato ? [end]\n",
      "\n",
      "Test 5:\n",
      "he rarely stays home on sunday .\n",
      "== [start] lui resta raramente a casa la domenica . [end]\n",
      "-> [start] lui sta raramente a casa la domenica . [end]\n",
      "\n",
      "Test 6:\n",
      "i am going to win this time .\n",
      "== [start] vincerò stavolta . [end]\n",
      "-> [start] io vincerà questa è è è è è è è è è è è è è è è è è\n",
      "\n",
      "Test 7:\n",
      "how did we come here ?\n",
      "== [start] noi come siamo venuti qui ? [end]\n",
      "-> [start] come siamo venuti qui ? [end]\n",
      "\n",
      "Test 8:\n",
      "we sent you a letter .\n",
      "== [start] ti abbiamo spedito una lettera . [end]\n",
      "-> [start] noi le abbiamo mandato una lettera . [end]\n",
      "\n",
      "Test 9:\n",
      "how much drinking water do we have left ?\n",
      "== [start] quanta acqua potabile ci rimane ? [end]\n",
      "-> [start] quanto acqua che ci resta ? [end]\n",
      "\n",
      "Test 10:\n",
      "did you notice any errors ?\n",
      "== [start] lei ha notato degli errori ? [end]\n",
      "-> [start] hai notato degli errori ? [end]\n",
      "\n",
      "Test 11:\n",
      "i bought it .\n",
      "== [start] lo comprai . [end]\n",
      "-> [start] l ho comprato . [end]\n",
      "\n",
      "Test 12:\n",
      "i will be very careful .\n",
      "== [start] io sarò molto prudente . [end]\n",
      "-> [start] sarò molto attenzione . [end]\n",
      "\n",
      "Test 13:\n",
      "how about going out for dinner ?\n",
      "== [start] che ne dite di andare fuori a cena ? [end]\n",
      "-> [start] che ne dite di uscire per cena ? [end]\n",
      "\n",
      "Test 14:\n",
      "i am afraid it is too late now .\n",
      "== [start] temo che sia troppo tardi adesso . [end]\n",
      "-> [start] temo che sia troppo tardi ora . [end]\n",
      "\n",
      "Test 15:\n",
      "would you excuse me ?\n",
      "== [start] tu mi scuseresti ? [end]\n",
      "-> [start] mi [UNK] ? [end]\n",
      "\n",
      "Test 16:\n",
      "the question is how long .\n",
      "== [start] la domanda è quanto a lungo . [end]\n",
      "-> [start] la domanda è molto tempo . [end]\n",
      "\n",
      "Test 17:\n",
      "were you drinking that day ?\n",
      "== [start] lei stava bevendo quel giorno ? [end]\n",
      "-> [start] voi stavate bevendo quel giorno ? [end]\n",
      "\n",
      "Test 18:\n",
      "that's my favorite kind of sandwich .\n",
      "== [start] quello è il mio tipo di sandwich preferito . [end]\n",
      "-> [start] è il mio tipo di sandwich preferito . [end]\n",
      "\n",
      "Test 19:\n",
      "tom did not see a thing .\n",
      "== [start] tom non ha visto assolutamente nulla . [end]\n",
      "-> [start] tom non vide niente . [end]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "test_count = 20\n",
    "random.seed(0)\n",
    "for n in range(test_count):\n",
    "    english_sentence, italian_sentence = random.choice(data[\"test\"])\n",
    "    translated = translate(english_sentence)\n",
    "    italian_sentence = re_sub_accented(italian_sentence)\n",
    "    #translated = re_sub_accented(translated)\n",
    "    print(f\"Test {n}:\")\n",
    "    print(f\"{english_sentence}\")\n",
    "    print(f\"== {italian_sentence}\")\n",
    "    print(f\"-> {' '.join(translated)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31438f3",
   "metadata": {},
   "source": [
    "The second line of each test is the expected output while the third line is the output from the transformer.\n",
    "\n",
    "The token [UNK] means “unknown” or out-of-vocabulary, which should appear rarely. Comparing the output, you should see the result is quite accurate. It will not be perfect. For example, ...\n",
    "\n",
    "You generated the translated sentence word by word, but indeed the transformer outputs the entire sentence in one shot. You should try to modify the program to decode the entire transformer output pred in the for-loop to see how the transformer gives you a better sentence as you provide more leading words in dec_tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
